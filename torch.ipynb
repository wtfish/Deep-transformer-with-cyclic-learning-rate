{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, einsum\n",
    "from torch.nn.modules.linear import Linear\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prelayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreLayer(nn.Module):\n",
    "    def __init__(self, hid, d_model, drop_out=0.0, in_dim=1):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_dim, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 100):\n",
    "        super().__init__()\n",
    "        # self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.squeeze(1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.pe[:x.size(1)]\n",
    "        # return self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_attention(query, key, value, causal=False, dropout=0.0):\n",
    "    device = key.device\n",
    "    B_k, h_k, n_k, d_k = key.shape\n",
    "    B_q, h_q, n_q, d_q = query.shape\n",
    "\n",
    "    scale = einsum(\"bhqd,bhkd->bhqk\", query, key)/math.sqrt(d_k)\n",
    "\n",
    "    if causal:\n",
    "        ones = torch.ones(B_k, h_k, n_q, n_k).to(device)\n",
    "        mask = torch.tril(ones)\n",
    "        scale = scale.masked_fill(mask == 0, -1e9)\n",
    "    atn = F.softmax(scale, dim=-1)\n",
    "    if dropout is not None:\n",
    "        atn = F.dropout(atn, p=dropout)   \n",
    "    out = einsum(\"bhqk,bhkd->bhqd\", atn, value)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_eachhead(x, head_num, split_num=3):\n",
    "    B, n, pre_d = x.shape\n",
    "    new_d = pre_d//split_num\n",
    "    assert pre_d%split_num == 0, f\"have to be multiple of {split_num}\"\n",
    "    assert new_d%head_num == 0, \"dim must be divided by head_num\"\n",
    "\n",
    "    tpl = torch.chunk(x, split_num, dim=2)\n",
    "    out = []\n",
    "    for t in tpl:\n",
    "        out.append(t.reshape(B, n, head_num, new_d//head_num).transpose(1,2))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_head(x):\n",
    "    B, h, n, _d = x.shape\n",
    "    out = x.transpose(1,2).reshape(B, n, _d*h)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MHSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
