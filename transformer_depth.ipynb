{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "from torch.nn.modules.linear import Linear\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreLayer(nn.Module):\n",
    "    def __init__(self, d_model, in_dim=1):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_dim, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostLayer(nn.Module):\n",
    "    def __init__(self, dim, vocab_num):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(dim, vocab_num)\n",
    "    def forward(self,x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len: int = 100):\n",
    "        super().__init__()\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.pe[:x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_attention(query, key, value, causal=False, dropout=0.0):\n",
    "    device = key.device\n",
    "    B_k, h_k, n_k, d_k = key.shape\n",
    "    B_q, h_q, n_q, d_q = query.shape\n",
    "\n",
    "    scale = einsum(\"bhqd,bhkd->bhqk\", query, key)/math.sqrt(d_k)\n",
    "    print(\"scale\",scale)\n",
    "\n",
    "    if causal:\n",
    "        ones = torch.ones(B_k, h_k, n_q, n_k).to(device)\n",
    "        mask = torch.tril(ones)\n",
    "        scale = scale.masked_fill(mask == 0, -1e9)\n",
    "        print(\"decoder masking\",scale)\n",
    "    atn = F.softmax(scale, dim=-1)\n",
    "    print(\"attention\",atn)\n",
    "    if dropout is not None:\n",
    "        atn = F.dropout(atn, p=dropout)   \n",
    "    out = einsum(\"bhqk,bhkd->bhqd\", atn, value)\n",
    "    print(\"out\",out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_eachhead(x, head_num, split_num=3):\n",
    "    B, n, pre_d = x.shape\n",
    "    new_d = pre_d//split_num\n",
    "    assert pre_d%split_num == 0, f\"have to be multiple of {split_num}\"\n",
    "    assert new_d%head_num == 0, \"dim must be divided by head_num\"\n",
    "\n",
    "    tpl = torch.chunk(x, split_num, dim=2)\n",
    "    out = []\n",
    "    for t in tpl:\n",
    "        out.append(t.reshape(B, n, head_num, new_d//head_num).transpose(1,2))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_head(x):\n",
    "    B, h, n, _d = x.shape\n",
    "    out = x.transpose(1,2).reshape(B, n, _d*h)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MHSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, dim, head_num):\n",
    "        super().__init__()\n",
    "        self.to_qvk = nn.Linear(dim, dim*3)\n",
    "        self.make_head = partial(to_eachhead, head_num=head_num, split_num=3)\n",
    "        self.mhsa = full_attention\n",
    "    \n",
    "    def forward(self, x):\n",
    "        qvk = self.to_qvk(x)\n",
    "        q, v, k = self.make_head(qvk)\n",
    "        out = self.mhsa(q, k, v)\n",
    "        out = concat_head(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking MHSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadCausalAttention(nn.Module):\n",
    "    def __init__(self, dim, head_num):\n",
    "        super().__init__()\n",
    "        self.to_qvk = nn.Linear(dim, dim*3)\n",
    "        self.make_head = partial(to_eachhead, head_num=head_num, split_num=3)\n",
    "        self.mhca = partial(full_attention, causal=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        qvk = self.to_qvk(x)\n",
    "        q, v, k = self.make_head(qvk)\n",
    "        out = self.mhca(q, k, v)\n",
    "        out = concat_head(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSourceAttention(nn.Module):\n",
    "    def __init__(self, dim, head_num):\n",
    "        super().__init__()\n",
    "        self.to_kv = nn.Linear(dim, dim*2)\n",
    "        self.to_q = nn.Linear(dim, dim)\n",
    "        self.make_head_kv = partial(to_eachhead, head_num=head_num, split_num=2)\n",
    "        self.make_head_q = partial(to_eachhead, head_num=head_num, split_num=1)\n",
    "        self.mhsa = full_attention\n",
    "\n",
    "    def forward(self, x, memory):\n",
    "        mem = self.to_kv(memory)\n",
    "        x = self.to_q(x)\n",
    "\n",
    "        k, v = self.make_head_kv(mem)\n",
    "        q = self.make_head_q(x)[0]\n",
    "        print(\"K matrix\",k)\n",
    "        print(\"v matrix\",v)\n",
    "        print(\"Q matrix\",q)\n",
    "        out = self.mhsa(q, k, v)\n",
    "        out = concat_head(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(dim, hid_dim, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hid_dim, dim, bias=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, dim, head_num, ff_hidnum, dropout_ratio, norm_first=False):\n",
    "        super().__init__()\n",
    "        self.dor = dropout_ratio\n",
    "        self.mhsa = MultiHeadSelfAttention(dim, head_num)\n",
    "        self.ln1 = nn.LayerNorm(dim)\n",
    "        self.ff = FeedForward(dim, ff_hidnum)\n",
    "        self.ln2 = nn.LayerNorm(dim)\n",
    "        self.norm_first = norm_first\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = torch.clone(x)\n",
    "\n",
    "        if self.norm_first:\n",
    "          out = self.ln1(x)\n",
    "          out = self.mhsa(out)\n",
    "          out = F.dropout(out, p=self.dor) + res\n",
    "\n",
    "          res = torch.clone(out)\n",
    "          out = self.ln2(out)\n",
    "          out = self.ff(out)\n",
    "          out = F.dropout(out, p=self.dor) + res\n",
    "        else:\n",
    "          out = self.mhsa(x)\n",
    "          out = F.dropout(out, p=self.dor) + res\n",
    "          print(\"Encoder residual 1\",out)\n",
    "          out = self.ln1(out)\n",
    "          print(\"LN1 ENC\",out)\n",
    "\n",
    "          res = torch.clone(out)\n",
    "          out = self.ff(out)\n",
    "          \n",
    "          out = F.dropout(out, p=self.dor) + res\n",
    "          print(\"Encoder residual 2\",out)\n",
    "          out = self.ln2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, depth, dim, head_num, ff_hidnum=2048, dropout_ratio=0.0, norm_first=False):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderLayer(dim, head_num, ff_hidnum, dropout_ratio, norm_first) for i in range(depth)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, dim, head_num, ff_hidnum, dropout_ratio, norm_first=False):\n",
    "        super().__init__()\n",
    "        self.dor = dropout_ratio\n",
    "        self.mhca = MultiHeadCausalAttention(dim, head_num)\n",
    "        self.ln1 = nn.LayerNorm(dim)\n",
    "        self.mhsa = MultiHeadSourceAttention(dim, head_num)\n",
    "        self.ln2 = nn.LayerNorm(dim)\n",
    "        self.ff = FeedForward(dim, ff_hidnum)\n",
    "        self.ln3 = nn.LayerNorm(dim)\n",
    "        self.norm_first = norm_first\n",
    "\n",
    "    def forward(self, x, memory):\n",
    "        res = torch.clone(x)\n",
    "\n",
    "        if self.norm_first:\n",
    "          out = self.ln1(x)\n",
    "          out = self.mhca(out)\n",
    "          out = F.dropout(out, p=self.dor) + res\n",
    "          \n",
    "          res = torch.clone(out)\n",
    "          out = self.ln2(out)\n",
    "          out = self.mhsa(out, memory)\n",
    "          out = F.dropout(out, p=self.dor) + res\n",
    "\n",
    "          res = torch.clone(out)\n",
    "          out = self.ln3(out)\n",
    "          out = self.ff(out)\n",
    "          out = F.dropout(out, p=self.dor) + res\n",
    "\n",
    "        else:\n",
    "          out = self.mhca(x)\n",
    "          out = F.dropout(out, p=self.dor) + res\n",
    "          print(\"Residual Connection Dec 1\",out)\n",
    "          out = self.ln1(out)\n",
    "\n",
    "          res = torch.clone(out)\n",
    "          out = self.mhsa(out, memory)\n",
    "          print(\"MHSA enc-dec\",out)\n",
    "          out = F.dropout(out, p=self.dor) + res\n",
    "          print(\"Residual Connection Dec 2\",out)\n",
    "          out = self.ln2(out)\n",
    "\n",
    "          res = torch.clone(out)\n",
    "          out = self.ff(out)\n",
    "          out = F.dropout(out, p=self.dor) + res\n",
    "          print(\"Residual Connection Dec 3\",out)\n",
    "          out = self.ln3(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, depth, dim, head_num, ff_hidnum, dropout_ratio=0.0, norm_first=False):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([DecoderLayer(dim, head_num, ff_hidnum, dropout_ratio, norm_first) for i in range(depth)])\n",
    "    \n",
    "    def forward(self, x, memory):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, device, d_model, in_dim, N_enc, N_dec, h_enc, h_dec, ff_hidnum,dropout_model=0, norm_first=False):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.x_pre = PreLayer(d_model,in_dim)\n",
    "        self.y_pre = PreLayer(d_model,in_dim)\n",
    "        self.pos = PositionalEncoding(d_model)\n",
    "        self.enc = Encoder(N_enc,d_model, h_enc, ff_hidnum, dropout_model, norm_first)\n",
    "        self.dec = Decoder(N_dec,d_model, h_dec, ff_hidnum, dropout_model, norm_first)\n",
    "        self.post = PostLayer(d_model, 1)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x_emb = self.x_pre(x)\n",
    "        y_emb = self.y_pre(y)\n",
    "        x_emb_pos = self.pos(x_emb)\n",
    "        y_emb_pos = self.pos(y_emb)\n",
    "        memory = self.enc(x_emb_pos)\n",
    "        out = self.dec(y_emb_pos, memory)\n",
    "        out = self.post(out)\n",
    "        out = out.squeeze(-1)\n",
    "        return out\n",
    "\n",
    "    def generate(self, x, forcast_step, y_start):\n",
    "        device = x.device\n",
    "        x = x.to(device)\n",
    "        B, N, D = x.shape\n",
    "        x = self.x_pre(x) \n",
    "        x = self.pos(x)\n",
    "        z = self.enc(x) \n",
    "        y = y_start\n",
    "        for i in range(forcast_step):\n",
    "            y_pred = self.y_pre(y)\n",
    "            y_pred = self.pos(y_pred)\n",
    "            y_pred = self.dec(y_pred, z)\n",
    "            y_pred = self.post(y_pred)\n",
    "            y = torch.cat([y, y_pred[:,[-1],:]], dim=1)\n",
    "        y_pred = y_pred.squeeze(-1)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ],\n",
       "       [-0.9436108 ],\n",
       "       [-0.51129353],\n",
       "       [-0.1228345 ],\n",
       "       [-1.        ],\n",
       "       [-0.9945804 ],\n",
       "       [-0.8878732 ]], dtype=float32)"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data_manual.csv\")\n",
    "data = data.to_numpy().astype(\"float32\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5, 1)\n"
     ]
    }
   ],
   "source": [
    "x = np.expand_dims(np.transpose(data[:5]),axis=-1)\n",
    "y = np.expand_dims(np.transpose(data[4:6]),axis=-1)\n",
    "tgt = np.expand_dims(np.transpose(data[5:]),axis=-1)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "x = np.expand_dims(np.transpose(data[:5]),axis=-1)\n",
    "y = np.expand_dims(np.transpose(data[4:6]),axis=-1)\n",
    "tgt = np.expand_dims(np.transpose(data[5:]),axis=-1)\n",
    "\n",
    "x_tensor = torch.tensor(x, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "tgt_tensor = torch.tensor(tgt, dtype=torch.float32)\n",
    "# Combine x, y, and tgt into a single dataset\n",
    "dataset = TensorDataset(x_tensor, y_tensor, tgt_tensor)\n",
    "\n",
    "# Create a DataLoader with the dataset\n",
    "batch_size = 1  # Set your desired batch size\n",
    "shuffle = False  # Set shuffle to True or False based on your requirements\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def fix_seed(seed):\n",
    "    # random\n",
    "    random.seed(seed)\n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "    # Pytorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "fix_seed(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Transformer(device, d_model=4,N_dec=1, N_enc=1, ff_hidnum=3, h_dec=1, h_enc=1,in_dim=1)\n",
    "net = net.to(device)\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_forward_result(name):\n",
    "    def hook(module, input, output):\n",
    "        print(f\"Forward result of {name}:\")\n",
    "        print(output)\n",
    "    return hook\n",
    "def print_backward_result(name):\n",
    "    def hook(module, input, output):\n",
    "        print(f\"Backward result of {name}:\")\n",
    "        print(output)\n",
    "    return hook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_hooks(module, name=\"\"):\n",
    "    for name, child in module.named_children():\n",
    "        if list(child.children()):\n",
    "            register_hooks(child, name)\n",
    "        else:\n",
    "            child.register_forward_hook(print_forward_result(name))\n",
    "register_hooks(net)\n",
    "\n",
    "def register_hooks_decoder(module, name=\"\"):\n",
    "    for name, child in module.named_modules():\n",
    "        if isinstance(child, nn.Module):\n",
    "            child.register_full_backward_hook(print_backward_result(name))\n",
    "\n",
    "register_hooks_decoder(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('x_pre.linear.weight', tensor([[-0.2682],\n",
      "        [ 0.4050],\n",
      "        [-0.3793],\n",
      "        [-0.9806]], device='cuda:0')), ('x_pre.linear.bias', tensor([ 0.3155, -0.6107,  0.9012,  0.3775], device='cuda:0')), ('y_pre.linear.weight', tensor([[0.6347],\n",
      "        [0.5150],\n",
      "        [0.4983],\n",
      "        [0.3748]], device='cuda:0')), ('y_pre.linear.bias', tensor([-0.4872, -0.8655,  0.8132,  0.8351], device='cuda:0')), ('pos.pe', tensor([[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
      "        [ 0.8415,  0.5403,  0.0100,  0.9999],\n",
      "        [ 0.9093, -0.4161,  0.0200,  0.9998],\n",
      "        [ 0.1411, -0.9900,  0.0300,  0.9996],\n",
      "        [-0.7568, -0.6536,  0.0400,  0.9992],\n",
      "        [-0.9589,  0.2837,  0.0500,  0.9988],\n",
      "        [-0.2794,  0.9602,  0.0600,  0.9982],\n",
      "        [ 0.6570,  0.7539,  0.0699,  0.9976],\n",
      "        [ 0.9894, -0.1455,  0.0799,  0.9968],\n",
      "        [ 0.4121, -0.9111,  0.0899,  0.9960],\n",
      "        [-0.5440, -0.8391,  0.0998,  0.9950],\n",
      "        [-1.0000,  0.0044,  0.1098,  0.9940],\n",
      "        [-0.5366,  0.8439,  0.1197,  0.9928],\n",
      "        [ 0.4202,  0.9074,  0.1296,  0.9916],\n",
      "        [ 0.9906,  0.1367,  0.1395,  0.9902],\n",
      "        [ 0.6503, -0.7597,  0.1494,  0.9888],\n",
      "        [-0.2879, -0.9577,  0.1593,  0.9872],\n",
      "        [-0.9614, -0.2752,  0.1692,  0.9856],\n",
      "        [-0.7510,  0.6603,  0.1790,  0.9838],\n",
      "        [ 0.1499,  0.9887,  0.1889,  0.9820],\n",
      "        [ 0.9129,  0.4081,  0.1987,  0.9801],\n",
      "        [ 0.8367, -0.5477,  0.2085,  0.9780],\n",
      "        [-0.0089, -1.0000,  0.2182,  0.9759],\n",
      "        [-0.8462, -0.5328,  0.2280,  0.9737],\n",
      "        [-0.9056,  0.4242,  0.2377,  0.9713],\n",
      "        [-0.1324,  0.9912,  0.2474,  0.9689],\n",
      "        [ 0.7626,  0.6469,  0.2571,  0.9664],\n",
      "        [ 0.9564, -0.2921,  0.2667,  0.9638],\n",
      "        [ 0.2709, -0.9626,  0.2764,  0.9611],\n",
      "        [-0.6636, -0.7481,  0.2860,  0.9582],\n",
      "        [-0.9880,  0.1543,  0.2955,  0.9553],\n",
      "        [-0.4040,  0.9147,  0.3051,  0.9523],\n",
      "        [ 0.5514,  0.8342,  0.3146,  0.9492],\n",
      "        [ 0.9999, -0.0133,  0.3240,  0.9460],\n",
      "        [ 0.5291, -0.8486,  0.3335,  0.9428],\n",
      "        [-0.4282, -0.9037,  0.3429,  0.9394],\n",
      "        [-0.9918, -0.1280,  0.3523,  0.9359],\n",
      "        [-0.6435,  0.7654,  0.3616,  0.9323],\n",
      "        [ 0.2964,  0.9551,  0.3709,  0.9287],\n",
      "        [ 0.9638,  0.2666,  0.3802,  0.9249],\n",
      "        [ 0.7451, -0.6669,  0.3894,  0.9211],\n",
      "        [-0.1586, -0.9873,  0.3986,  0.9171],\n",
      "        [-0.9165, -0.4000,  0.4078,  0.9131],\n",
      "        [-0.8318,  0.5551,  0.4169,  0.9090],\n",
      "        [ 0.0177,  0.9998,  0.4259,  0.9048],\n",
      "        [ 0.8509,  0.5253,  0.4350,  0.9004],\n",
      "        [ 0.9018, -0.4322,  0.4439,  0.8961],\n",
      "        [ 0.1236, -0.9923,  0.4529,  0.8916],\n",
      "        [-0.7683, -0.6401,  0.4618,  0.8870],\n",
      "        [-0.9538,  0.3006,  0.4706,  0.8823],\n",
      "        [-0.2624,  0.9650,  0.4794,  0.8776],\n",
      "        [ 0.6702,  0.7422,  0.4882,  0.8727],\n",
      "        [ 0.9866, -0.1630,  0.4969,  0.8678],\n",
      "        [ 0.3959, -0.9183,  0.5055,  0.8628],\n",
      "        [-0.5588, -0.8293,  0.5141,  0.8577],\n",
      "        [-0.9998,  0.0221,  0.5227,  0.8525],\n",
      "        [-0.5216,  0.8532,  0.5312,  0.8473],\n",
      "        [ 0.4362,  0.8999,  0.5396,  0.8419],\n",
      "        [ 0.9929,  0.1192,  0.5480,  0.8365],\n",
      "        [ 0.6367, -0.7711,  0.5564,  0.8309],\n",
      "        [-0.3048, -0.9524,  0.5646,  0.8253],\n",
      "        [-0.9661, -0.2581,  0.5729,  0.8196],\n",
      "        [-0.7392,  0.6735,  0.5810,  0.8139],\n",
      "        [ 0.1674,  0.9859,  0.5891,  0.8080],\n",
      "        [ 0.9200,  0.3919,  0.5972,  0.8021],\n",
      "        [ 0.8268, -0.5625,  0.6052,  0.7961],\n",
      "        [-0.0266, -0.9996,  0.6131,  0.7900],\n",
      "        [-0.8555, -0.5178,  0.6210,  0.7838],\n",
      "        [-0.8979,  0.4401,  0.6288,  0.7776],\n",
      "        [-0.1148,  0.9934,  0.6365,  0.7712],\n",
      "        [ 0.7739,  0.6333,  0.6442,  0.7648],\n",
      "        [ 0.9511, -0.3090,  0.6518,  0.7584],\n",
      "        [ 0.2538, -0.9673,  0.6594,  0.7518],\n",
      "        [-0.6768, -0.7362,  0.6669,  0.7452],\n",
      "        [-0.9851,  0.1717,  0.6743,  0.7385],\n",
      "        [-0.3878,  0.9218,  0.6816,  0.7317],\n",
      "        [ 0.5661,  0.8243,  0.6889,  0.7248],\n",
      "        [ 0.9995, -0.0310,  0.6961,  0.7179],\n",
      "        [ 0.5140, -0.8578,  0.7033,  0.7109],\n",
      "        [-0.4441, -0.8960,  0.7104,  0.7038],\n",
      "        [-0.9939, -0.1104,  0.7174,  0.6967],\n",
      "        [-0.6299,  0.7767,  0.7243,  0.6895],\n",
      "        [ 0.3132,  0.9497,  0.7311,  0.6822],\n",
      "        [ 0.9684,  0.2495,  0.7379,  0.6749],\n",
      "        [ 0.7332, -0.6800,  0.7446,  0.6675],\n",
      "        [-0.1761, -0.9844,  0.7513,  0.6600],\n",
      "        [-0.9235, -0.3837,  0.7578,  0.6524],\n",
      "        [-0.8218,  0.5698,  0.7643,  0.6448],\n",
      "        [ 0.0354,  0.9994,  0.7707,  0.6372],\n",
      "        [ 0.8601,  0.5102,  0.7771,  0.6294],\n",
      "        [ 0.8940, -0.4481,  0.7833,  0.6216],\n",
      "        [ 0.1060, -0.9944,  0.7895,  0.6137],\n",
      "        [-0.7795, -0.6264,  0.7956,  0.6058],\n",
      "        [-0.9483,  0.3174,  0.8016,  0.5978],\n",
      "        [-0.2453,  0.9695,  0.8076,  0.5898],\n",
      "        [ 0.6833,  0.7302,  0.8134,  0.5817],\n",
      "        [ 0.9836, -0.1804,  0.8192,  0.5735],\n",
      "        [ 0.3796, -0.9251,  0.8249,  0.5653],\n",
      "        [-0.5734, -0.8193,  0.8305,  0.5570],\n",
      "        [-0.9992,  0.0398,  0.8360,  0.5487]], device='cuda:0')), ('enc.layers.0.mhsa.to_qvk.weight', tensor([[-0.0764,  0.2978,  0.3594, -0.1340],\n",
      "        [ 0.1949, -0.1657,  0.1681, -0.0049],\n",
      "        [-0.1861,  0.2763,  0.2457,  0.0009],\n",
      "        [-0.0875, -0.3292,  0.0721, -0.4952],\n",
      "        [ 0.1432, -0.0479, -0.2950, -0.3609],\n",
      "        [ 0.1961, -0.4320, -0.3200, -0.0140],\n",
      "        [ 0.1061, -0.3830, -0.3161, -0.1266],\n",
      "        [-0.3810, -0.1964, -0.0073,  0.1670],\n",
      "        [-0.0459,  0.4190,  0.0781, -0.4970],\n",
      "        [ 0.3747, -0.2191,  0.1452,  0.0655],\n",
      "        [-0.1429, -0.4419, -0.3451,  0.1499],\n",
      "        [-0.0828,  0.2136,  0.0622, -0.3256]], device='cuda:0')), ('enc.layers.0.mhsa.to_qvk.bias', tensor([-0.0565,  0.4533,  0.0063, -0.1738,  0.4815,  0.1465,  0.2026, -0.1836,\n",
      "        -0.4289,  0.3744,  0.3535,  0.0467], device='cuda:0')), ('enc.layers.0.ln1.weight', tensor([1., 1., 1., 1.], device='cuda:0')), ('enc.layers.0.ln1.bias', tensor([0., 0., 0., 0.], device='cuda:0')), ('enc.layers.0.ff.linear1.weight', tensor([[ 0.4768,  0.4949, -0.4620,  0.1607],\n",
      "        [ 0.1838, -0.0346,  0.1894, -0.3263],\n",
      "        [-0.4538, -0.3455,  0.0465, -0.1984]], device='cuda:0')), ('enc.layers.0.ff.linear1.bias', tensor([ 0.0738,  0.2364, -0.1838], device='cuda:0')), ('enc.layers.0.ff.linear2.weight', tensor([[-0.2365, -0.1237,  0.4150],\n",
      "        [-0.5161, -0.2100, -0.5307],\n",
      "        [ 0.0281, -0.4580,  0.2137],\n",
      "        [ 0.0866, -0.2388, -0.4672]], device='cuda:0')), ('enc.layers.0.ff.linear2.bias', tensor([-0.3809, -0.0024,  0.1790,  0.0416], device='cuda:0')), ('enc.layers.0.ln2.weight', tensor([1., 1., 1., 1.], device='cuda:0')), ('enc.layers.0.ln2.bias', tensor([0., 0., 0., 0.], device='cuda:0')), ('dec.layers.0.mhca.to_qvk.weight', tensor([[ 0.0379, -0.1005,  0.0657, -0.0074],\n",
      "        [ 0.4559,  0.4950, -0.4763,  0.3770],\n",
      "        [ 0.3806,  0.4240, -0.0830, -0.2643],\n",
      "        [ 0.2191, -0.1465,  0.4139, -0.4185],\n",
      "        [ 0.1251, -0.0970,  0.2131, -0.4008],\n",
      "        [-0.1877,  0.0866,  0.2668, -0.3937],\n",
      "        [ 0.4077,  0.1085, -0.3414, -0.0505],\n",
      "        [-0.3877,  0.2640,  0.2076, -0.0888],\n",
      "        [ 0.2840,  0.0567, -0.0864, -0.4940],\n",
      "        [-0.1811, -0.4052,  0.3763,  0.4005],\n",
      "        [-0.2731, -0.2079, -0.1680, -0.4867],\n",
      "        [-0.0972, -0.0581, -0.4126,  0.4835]], device='cuda:0')), ('dec.layers.0.mhca.to_qvk.bias', tensor([-0.4087, -0.0889,  0.1709,  0.4121, -0.3655, -0.1204, -0.1336, -0.1922,\n",
      "         0.0753, -0.4092, -0.1540,  0.3804], device='cuda:0')), ('dec.layers.0.ln1.weight', tensor([1., 1., 1., 1.], device='cuda:0')), ('dec.layers.0.ln1.bias', tensor([0., 0., 0., 0.], device='cuda:0')), ('dec.layers.0.mhsa.to_kv.weight', tensor([[ 0.2841,  0.4436, -0.1006, -0.0301],\n",
      "        [ 0.1842,  0.1945, -0.0551,  0.1102],\n",
      "        [ 0.0151, -0.0109, -0.2432,  0.2961],\n",
      "        [-0.4394, -0.2283,  0.0326,  0.3265],\n",
      "        [ 0.2905,  0.1756, -0.3365,  0.2573],\n",
      "        [ 0.3001,  0.3455,  0.3918, -0.2927],\n",
      "        [ 0.3862,  0.2865,  0.4065,  0.2640],\n",
      "        [ 0.3821,  0.3359,  0.3989, -0.1817]], device='cuda:0')), ('dec.layers.0.mhsa.to_kv.bias', tensor([ 0.3991, -0.2893, -0.4332,  0.4100,  0.2216,  0.2787, -0.3241,  0.3215],\n",
      "       device='cuda:0')), ('dec.layers.0.mhsa.to_q.weight', tensor([[-0.4688, -0.2195, -0.0746, -0.2068],\n",
      "        [ 0.2856,  0.4543,  0.2394, -0.1444],\n",
      "        [-0.0258, -0.2001, -0.1581, -0.1730],\n",
      "        [ 0.4009, -0.1401,  0.1241,  0.0197]], device='cuda:0')), ('dec.layers.0.mhsa.to_q.bias', tensor([-0.1663, -0.2557, -0.3440,  0.2109], device='cuda:0')), ('dec.layers.0.ln2.weight', tensor([1., 1., 1., 1.], device='cuda:0')), ('dec.layers.0.ln2.bias', tensor([0., 0., 0., 0.], device='cuda:0')), ('dec.layers.0.ff.linear1.weight', tensor([[ 0.2631, -0.3788,  0.1737, -0.3783],\n",
      "        [-0.2449, -0.1366,  0.1374,  0.4378],\n",
      "        [ 0.3584, -0.1433,  0.2044,  0.2551]], device='cuda:0')), ('dec.layers.0.ff.linear1.bias', tensor([ 0.4331,  0.2419, -0.3063], device='cuda:0')), ('dec.layers.0.ff.linear2.weight', tensor([[ 0.3878,  0.5672, -0.1952],\n",
      "        [ 0.2749,  0.1378, -0.4101],\n",
      "        [ 0.5297, -0.1215,  0.4580],\n",
      "        [ 0.3199,  0.0376,  0.4590]], device='cuda:0')), ('dec.layers.0.ff.linear2.bias', tensor([ 0.4910,  0.2968, -0.2329, -0.3475], device='cuda:0')), ('dec.layers.0.ln3.weight', tensor([1., 1., 1., 1.], device='cuda:0')), ('dec.layers.0.ln3.bias', tensor([0., 0., 0., 0.], device='cuda:0')), ('post.linear.weight', tensor([[ 0.0592,  0.4870,  0.4021, -0.3149]], device='cuda:0')), ('post.linear.bias', tensor([-0.0633], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "before_backprop = net.state_dict()\n",
    "print(before_backprop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward result of linear:\n",
      "tensor([[[ 0.0473, -0.2056,  0.5219, -0.6031],\n",
      "         [ 0.5686, -0.9929,  1.2591,  1.3028],\n",
      "         [ 0.4526, -0.8178,  1.0951,  0.8789],\n",
      "         [ 0.3484, -0.6604,  0.9478,  0.4980],\n",
      "         [ 0.5837, -1.0157,  1.2805,  1.3581]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Forward result of linear:\n",
      "tensor([[[-1.1219, -1.3805,  0.3148,  0.4602],\n",
      "         [-1.1185, -1.3777,  0.3175,  0.4623]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Forward result of pos:\n",
      "tensor([[[ 0.0473,  0.7944,  0.5219,  0.3969],\n",
      "         [ 1.4100, -0.4526,  1.2691,  2.3028],\n",
      "         [ 1.3619, -1.2339,  1.1151,  1.8787],\n",
      "         [ 0.4895, -1.6504,  0.9778,  1.4975],\n",
      "         [-0.1731, -1.6694,  1.3205,  2.3573]]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Forward result of pos:\n",
      "tensor([[[-1.1219, -0.3805,  0.3148,  1.4602],\n",
      "         [-0.2770, -0.8374,  0.3275,  1.4622]]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Forward result of to_qvk:\n",
      "tensor([[[ 0.3109,  0.4167,  0.3456, -0.5983,  0.1531, -0.3599, -0.3118,\n",
      "          -0.2951, -0.2547,  0.3198, -0.1249,  0.1157],\n",
      "         [-0.1513,  1.0052, -0.0672, -1.1970, -0.5002,  0.1802, -0.1670,\n",
      "          -0.2567, -1.7285,  1.3370,  0.2593, -0.8376],\n",
      "         [-0.3788,  1.1015, -0.3123, -0.7367, -0.2712,  0.5635,  0.2294,\n",
      "          -0.1546, -1.8550,  1.4401,  0.6010, -0.8719],\n",
      "         [-0.4345,  0.9793, -0.2991, -0.3444, -0.1981,  0.6216,  0.3880,\n",
      "           0.1970, -1.8108,  1.1595,  0.8999, -0.7731],\n",
      "         [-0.3815,  0.9067, -0.0961, -0.6813, -0.7035,  0.3782,  0.1078,\n",
      "           0.5943, -2.1889,  1.0215,  1.0137, -0.9808]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "scale tensor([[[[-0.0292,  0.3053,  0.3764,  0.3469,  0.3412],\n",
      "          [ 0.1150,  1.2954,  1.3659,  1.1523,  1.2321],\n",
      "          [ 0.2013,  1.3318,  1.3718,  1.1259,  1.1802],\n",
      "          [ 0.2107,  1.1356,  1.1684,  0.9597,  0.9930],\n",
      "          [ 0.1601,  1.2087,  1.2748,  1.0912,  1.1660]]]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "attention tensor([[[[0.1470, 0.2054, 0.2205, 0.2141, 0.2129],\n",
      "          [0.0734, 0.2389, 0.2564, 0.2071, 0.2243],\n",
      "          [0.0800, 0.2477, 0.2578, 0.2016, 0.2129],\n",
      "          [0.0959, 0.2418, 0.2499, 0.2028, 0.2097],\n",
      "          [0.0821, 0.2344, 0.2504, 0.2084, 0.2246]]]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "out tensor([[[[-0.3322,  0.3220,  0.0765,  0.0385],\n",
      "          [-0.3766,  0.3747,  0.1006,  0.0515],\n",
      "          [-0.3713,  0.3670,  0.0940,  0.0392],\n",
      "          [-0.3617,  0.3552,  0.0883,  0.0356],\n",
      "          [-0.3719,  0.3683,  0.0978,  0.0514]]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Encoder residual 1 tensor([[[-0.2850,  1.1164,  0.5984,  0.4354],\n",
      "         [ 1.0335, -0.0779,  1.3696,  2.3542],\n",
      "         [ 0.9906, -0.8669,  1.2091,  1.9179],\n",
      "         [ 0.1278, -1.2952,  1.0661,  1.5331],\n",
      "         [-0.5450, -1.3011,  1.4182,  2.4087]]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Forward result of ln1:\n",
      "tensor([[[-1.4985,  1.2966,  0.2634, -0.0616],\n",
      "         [-0.1570, -1.4364,  0.2300,  1.3635],\n",
      "         [ 0.1730, -1.6330,  0.3855,  1.0745],\n",
      "         [-0.2130, -1.5303,  0.6555,  1.0878],\n",
      "         [-0.7004, -1.2095,  0.6215,  1.2885]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "LN1 ENC tensor([[[-1.4985,  1.2966,  0.2634, -0.0616],\n",
      "         [-0.1570, -1.4364,  0.2300,  1.3635],\n",
      "         [ 0.1730, -1.6330,  0.3855,  1.0745],\n",
      "         [-0.2130, -1.5303,  0.6555,  1.0878],\n",
      "         [-0.7004, -1.2095,  0.6215,  1.2885]]], device='cuda:0',\n",
      "       grad_fn=<BackwardHookFunctionBackward>)\n",
      "Forward result of linear1:\n",
      "tensor([[[-0.1305, -0.0138,  0.0725],\n",
      "         [-0.5992, -0.1441,  0.1239],\n",
      "         [-0.6574,  0.0470,  0.1066],\n",
      "         [-0.9132,  0.0193,  0.2562],\n",
      "         [-0.9388, -0.1532,  0.3252]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Forward result of relu:\n",
      "tensor([[[0.0000, 0.0000, 0.0725],\n",
      "         [0.0000, 0.0000, 0.1239],\n",
      "         [0.0000, 0.0470, 0.1066],\n",
      "         [0.0000, 0.0193, 0.2562],\n",
      "         [0.0000, 0.0000, 0.3252]]], device='cuda:0', grad_fn=<ReluBackward0>)\n",
      "Forward result of linear2:\n",
      "tensor([[[-0.3508, -0.0409,  0.1945,  0.0077],\n",
      "         [-0.3294, -0.0681,  0.2055, -0.0163],\n",
      "         [-0.3424, -0.0688,  0.1802, -0.0194],\n",
      "         [-0.2769, -0.1424,  0.2249, -0.0827],\n",
      "         [-0.2459, -0.1749,  0.2485, -0.1103]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Encoder residual 2 tensor([[[-1.8492,  1.2558,  0.4579, -0.0539],\n",
      "         [-0.4865, -1.5046,  0.4355,  1.3472],\n",
      "         [-0.1694, -1.7019,  0.5657,  1.0551],\n",
      "         [-0.4899, -1.6727,  0.8804,  1.0050],\n",
      "         [-0.9464, -1.3845,  0.8700,  1.1781]]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Forward result of ln2:\n",
      "tensor([[[-1.5803,  1.1429,  0.4431, -0.0057],\n",
      "         [-0.4098, -1.3703,  0.4600,  1.3201],\n",
      "         [-0.1025, -1.5732,  0.6030,  1.0727],\n",
      "         [-0.3838, -1.4631,  0.8666,  0.9803],\n",
      "         [-0.7882, -1.1825,  0.8467,  1.1240]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "Forward result of to_qvk:\n",
      "tensor([[[-0.4031, -0.3881, -0.8295, -0.2588, -0.9872, -0.4337, -0.8136,\n",
      "           0.0780, -1.0136,  0.6515, -0.5321,  1.0877],\n",
      "         [-0.3243, -0.2344, -0.7032, -0.0023, -0.8352, -0.6292, -0.5231,\n",
      "          -0.3678, -0.8016,  0.6893, -0.6710,  1.0278]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "scale tensor([[[[0.1578, 0.1731],\n",
      "          [0.2738, 0.2839]]]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "decoder masking tensor([[[[ 1.5779e-01, -1.0000e+09],\n",
      "          [ 2.7381e-01,  2.8390e-01]]]], device='cuda:0',\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "attention tensor([[[[1.0000, 0.0000],\n",
      "          [0.4975, 0.5025]]]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "out tensor([[[[-0.9872, -0.4337, -0.8136,  0.0780],\n",
      "          [-0.9108, -0.5319, -0.6676, -0.1460]]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Residual Connection Dec 1 tensor([[[-2.1091, -0.8142, -0.4987,  1.5382],\n",
      "         [-1.1878, -1.3693, -0.3401,  1.3162]]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Forward result of ln1:\n",
      "tensor([[[-1.2528, -0.2625, -0.0213,  1.5366],\n",
      "         [-0.7465, -0.9174,  0.0520,  1.6119]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "Forward result of to_kv:\n",
      "tensor([[[ 0.4127, -0.3833, -0.5790,  0.8560, -0.1874,  0.3747, -0.4283,\n",
      "           0.2795],\n",
      "         [-0.4111, -0.5112, -0.1454,  1.3489,  0.0467, -0.5239, -0.3394,\n",
      "          -0.3517],\n",
      "         [-0.4208, -0.5292, -0.2466,  1.1841, -0.0115, -0.3733, -0.2860,\n",
      "          -0.2005],\n",
      "         [-0.4756, -0.5843, -0.3435,  1.2610, -0.1863, -0.2894, -0.2804,\n",
      "          -0.1491],\n",
      "         [-0.4683, -0.5873, -0.3053,  1.4209, -0.2108, -0.3636, -0.3263,\n",
      "          -0.2433]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Forward result of to_q:\n",
      "tensor([[[ 0.1625, -0.9599, -0.5218, -0.2270],\n",
      "         [ 0.0479, -1.1061, -0.4284,  0.0784]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "K matrix tensor([[[[ 0.4127, -0.3833, -0.5790,  0.8560],\n",
      "          [-0.4111, -0.5112, -0.1454,  1.3489],\n",
      "          [-0.4208, -0.5292, -0.2466,  1.1841],\n",
      "          [-0.4756, -0.5843, -0.3435,  1.2610],\n",
      "          [-0.4683, -0.5873, -0.3053,  1.4209]]]], device='cuda:0',\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "v matrix tensor([[[[-0.1874,  0.3747, -0.4283,  0.2795],\n",
      "          [ 0.0467, -0.5239, -0.3394, -0.3517],\n",
      "          [-0.0115, -0.3733, -0.2860, -0.2005],\n",
      "          [-0.1863, -0.2894, -0.2804, -0.1491],\n",
      "          [-0.2108, -0.3636, -0.3263, -0.2433]]]], device='cuda:0',\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "Q matrix tensor([[[[ 0.1625, -0.9599, -0.5218, -0.2270],\n",
      "          [ 0.0479, -1.1061, -0.4284,  0.0784]]]], device='cuda:0',\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "scale tensor([[[[0.2714, 0.0968, 0.1497, 0.1883, 0.1622],\n",
      "          [0.3794, 0.3569, 0.3818, 0.4347, 0.4347]]]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "attention tensor([[[[0.2202, 0.1849, 0.1949, 0.2026, 0.1974],\n",
      "          [0.1963, 0.1919, 0.1968, 0.2075, 0.2075]]]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "out tensor([[[[-0.1142, -0.2176, -0.3340, -0.1208],\n",
      "          [-0.1125, -0.2359, -0.3314, -0.1335]]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "MHSA enc-dec tensor([[[-0.1142, -0.2176, -0.3340, -0.1208],\n",
      "         [-0.1125, -0.2359, -0.3314, -0.1335]]], device='cuda:0',\n",
      "       grad_fn=<BackwardHookFunctionBackward>)\n",
      "Residual Connection Dec 2 tensor([[[-1.3671, -0.4801, -0.3553,  1.4158],\n",
      "         [-0.8590, -1.1534, -0.2794,  1.4784]]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Forward result of ln2:\n",
      "tensor([[[-1.1595, -0.2808, -0.1572,  1.5975],\n",
      "         [-0.6424, -0.9309, -0.0746,  1.6478]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "Forward result of linear1:\n",
      "tensor([[[-0.3972,  1.2420, -0.3061],\n",
      "         [-0.0196,  1.2375,  0.0021]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Forward result of relu:\n",
      "tensor([[[0.0000, 1.2420, 0.0000],\n",
      "         [0.0000, 1.2375, 0.0021]]], device='cuda:0', grad_fn=<ReluBackward0>)\n",
      "Forward result of linear2:\n",
      "tensor([[[ 1.1955,  0.4679, -0.3838, -0.3007],\n",
      "         [ 1.1925,  0.4664, -0.3823, -0.3000]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Residual Connection Dec 3 tensor([[[ 0.0359,  0.1872, -0.5409,  1.2967],\n",
      "         [ 0.5502, -0.4644, -0.4568,  1.3478]]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "Forward result of ln3:\n",
      "tensor([[[-0.3138, -0.0865, -1.1808,  1.5811],\n",
      "         [ 0.4030, -0.9334, -0.9234,  1.4538]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "Forward result of linear:\n",
      "tensor([[[-1.0967],\n",
      "         [-1.3232]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Backward result of :\n",
      "(tensor([[-0.1021, -0.4353]], device='cuda:0'),)\n",
      "Backward result of post.linear:\n",
      "(tensor([[[-0.1021],\n",
      "         [-0.4353]]], device='cuda:0'),)\n",
      "Backward result of post:\n",
      "(tensor([[[-0.1021],\n",
      "         [-0.4353]]], device='cuda:0'),)\n",
      "Backward result of dec.layers.0.ln3:\n",
      "(tensor([[[-0.0060, -0.0497, -0.0411,  0.0322],\n",
      "         [-0.0258, -0.2120, -0.1750,  0.1371]]], device='cuda:0'),)\n",
      "Backward result of dec.layers.0.ff.linear2:\n",
      "(tensor([[[ 0.0277, -0.0470,  0.0094,  0.0099],\n",
      "         [-0.0159, -0.0199,  0.0270,  0.0088]]], device='cuda:0'),)\n",
      "Backward result of dec.layers.0.ff.relu:\n",
      "(tensor([[[ 0.0060,  0.0084,  0.0228],\n",
      "         [ 0.0055, -0.0147,  0.0277]]], device='cuda:0'),)\n",
      "Backward result of dec.layers.0.ff.linear1:\n",
      "(tensor([[[ 0.0000,  0.0084,  0.0000],\n",
      "         [ 0.0000, -0.0147,  0.0277]]], device='cuda:0'),)\n",
      "Backward result of dec.layers.0.ff:\n",
      "(tensor([[[ 0.0277, -0.0470,  0.0094,  0.0099],\n",
      "         [-0.0159, -0.0199,  0.0270,  0.0088]]], device='cuda:0'),)\n",
      "Backward result of dec.layers.0.ln2:\n",
      "(tensor([[[ 0.0256, -0.0482,  0.0106,  0.0136],\n",
      "         [-0.0024, -0.0218,  0.0306,  0.0094]]], device='cuda:0'),)\n",
      "Backward result of dec.layers.0.mhsa.to_q:\n",
      "(tensor([[[-0.0025, -0.0004,  0.0011,  0.0013],\n",
      "         [-0.0014, -0.0003,  0.0006,  0.0007]]], device='cuda:0'),)\n",
      "Backward result of dec.layers.0.mhsa.to_kv:\n",
      "(tensor([[[-5.5450e-04,  4.6268e-03,  2.2312e-03,  5.3593e-04,  5.6067e-03,\n",
      "          -1.3925e-02,  7.5091e-03,  8.0947e-04],\n",
      "         [ 2.7266e-04, -2.1293e-03, -1.0485e-03, -2.8925e-04,  4.6893e-03,\n",
      "          -1.2162e-02,  7.0315e-03,  4.4108e-04],\n",
      "         [ 1.7315e-04, -1.3553e-03, -6.6686e-04, -1.8313e-04,  4.9483e-03,\n",
      "          -1.2727e-02,  7.2641e-03,  5.1436e-04],\n",
      "         [ 4.1615e-05, -4.5752e-04, -2.0424e-04, -2.0779e-05,  5.1407e-03,\n",
      "          -1.3278e-02,  7.6290e-03,  5.0845e-04],\n",
      "         [ 6.7070e-05, -6.8468e-04, -3.1160e-04, -4.2778e-05,  5.0045e-03,\n",
      "          -1.3028e-02,  7.5752e-03,  4.4841e-04]]], device='cuda:0'),)\n",
      "Backward result of dec.layers.0.mhsa:\n",
      "(tensor([[[ 0.0261, -0.0479,  0.0102,  0.0115],\n",
      "         [-0.0007, -0.0173,  0.0268, -0.0088]]], device='cuda:0'),)\n",
      "Backward result of dec.layers.0.ln1:\n",
      "(tensor([[[ 0.0276, -0.0479,  0.0103,  0.0119],\n",
      "         [ 0.0002, -0.0173,  0.0268, -0.0086]]], device='cuda:0'),)\n",
      "Backward result of dec.layers.0.mhca.to_qvk:\n",
      "(tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.0050e-02,\n",
      "          -4.5084e-02,  1.9904e-02,  5.1295e-03,  5.9578e-04,  4.3069e-04,\n",
      "           1.2919e-03,  4.2978e-06],\n",
      "         [ 3.8952e-04,  6.9356e-05, -2.5514e-04, -1.0997e-04,  2.3458e-04,\n",
      "          -7.9566e-03,  1.2546e-02, -4.8244e-03, -5.9578e-04, -4.3069e-04,\n",
      "          -1.2919e-03, -4.2978e-06]]], device='cuda:0'),)\n",
      "Backward result of dec.layers.0.mhca:\n",
      "(tensor([[[ 0.0198, -0.0372,  0.0075,  0.0099],\n",
      "         [ 0.0005, -0.0158,  0.0250, -0.0096]]], device='cuda:0'),)\n",
      "Backward result of dec.layers.0:\n",
      "(tensor([[[-0.0060, -0.0497, -0.0411,  0.0322],\n",
      "         [-0.0258, -0.2120, -0.1750,  0.1371]]], device='cuda:0'),)\n",
      "Backward result of dec:\n",
      "(tensor([[[-0.0060, -0.0497, -0.0411,  0.0322],\n",
      "         [-0.0258, -0.2120, -0.1750,  0.1371]]], device='cuda:0'),)\n",
      "Backward result of enc.layers.0.ln2:\n",
      "(tensor([[[ 0.0012, -0.0009, -0.0047,  0.0087],\n",
      "         [ 0.0004, -0.0014, -0.0030,  0.0059],\n",
      "         [ 0.0005, -0.0014, -0.0033,  0.0064],\n",
      "         [ 0.0006, -0.0014, -0.0036,  0.0070],\n",
      "         [ 0.0005, -0.0014, -0.0034,  0.0068]]], device='cuda:0'),)\n",
      "Backward result of enc.layers.0.ff.linear2:\n",
      "(tensor([[[-1.6515e-03, -4.7799e-04, -4.5699e-03,  6.6994e-03],\n",
      "         [ 7.2122e-04,  8.5992e-04, -4.1405e-03,  2.5594e-03],\n",
      "         [ 1.1484e-04,  7.8344e-04, -4.7010e-03,  3.8027e-03],\n",
      "         [ 4.1982e-04, -3.5781e-06, -4.9580e-03,  4.5418e-03],\n",
      "         [ 9.7990e-04, -2.3483e-04, -4.8030e-03,  4.0579e-03]]],\n",
      "       device='cuda:0'),)\n",
      "Backward result of enc.layers.0.ff.relu:\n",
      "(tensor([[[ 0.0011,  0.0008, -0.0045],\n",
      "         [-0.0005,  0.0010, -0.0022],\n",
      "         [-0.0002,  0.0011, -0.0031],\n",
      "         [ 0.0002,  0.0011, -0.0030],\n",
      "         [ 0.0001,  0.0012, -0.0024]]], device='cuda:0'),)\n",
      "Backward result of enc.layers.0.ff.linear1:\n",
      "(tensor([[[ 0.0000,  0.0000, -0.0045],\n",
      "         [ 0.0000,  0.0000, -0.0022],\n",
      "         [ 0.0000,  0.0011, -0.0031],\n",
      "         [ 0.0000,  0.0011, -0.0030],\n",
      "         [ 0.0000,  0.0000, -0.0024]]], device='cuda:0'),)\n",
      "Backward result of enc.layers.0.ff:\n",
      "(tensor([[[-1.6515e-03, -4.7799e-04, -4.5699e-03,  6.6994e-03],\n",
      "         [ 7.2122e-04,  8.5992e-04, -4.1405e-03,  2.5594e-03],\n",
      "         [ 1.1484e-04,  7.8344e-04, -4.7010e-03,  3.8027e-03],\n",
      "         [ 4.1982e-04, -3.5781e-06, -4.9580e-03,  4.5418e-03],\n",
      "         [ 9.7990e-04, -2.3483e-04, -4.8030e-03,  4.0579e-03]]],\n",
      "       device='cuda:0'),)\n",
      "Backward result of enc.layers.0.ln1:\n",
      "(tensor([[[ 0.0004,  0.0011, -0.0048,  0.0076],\n",
      "         [ 0.0017,  0.0016, -0.0042,  0.0030],\n",
      "         [ 0.0017,  0.0018, -0.0046,  0.0041],\n",
      "         [ 0.0020,  0.0010, -0.0049,  0.0048],\n",
      "         [ 0.0021,  0.0006, -0.0049,  0.0045]]], device='cuda:0'),)\n",
      "Backward result of enc.layers.0.mhsa.to_qvk:\n",
      "(tensor([[[-4.3580e-04, -1.8060e-04,  3.7654e-04, -1.9904e-04,  8.4096e-05,\n",
      "           3.1784e-04, -3.3316e-03,  2.9297e-03, -9.0154e-05,  1.3595e-04,\n",
      "          -6.6698e-05, -6.6173e-05],\n",
      "         [ 2.5377e-05, -7.1815e-05, -1.0276e-05,  2.0500e-05,  6.9754e-04,\n",
      "           8.2889e-04, -7.1307e-03,  5.6042e-03, -3.5473e-05,  1.1742e-05,\n",
      "          -3.3422e-05,  4.7754e-06],\n",
      "         [-5.3941e-07, -8.3916e-05,  1.7538e-05,  9.6352e-06,  7.2795e-04,\n",
      "           8.7869e-04, -7.5573e-03,  5.9507e-03,  3.3071e-05, -7.0747e-04,\n",
      "          -5.4219e-05,  6.3744e-04],\n",
      "         [ 3.7872e-05, -1.1794e-04,  3.4173e-06,  3.4476e-05,  5.1627e-04,\n",
      "           7.2645e-04, -6.5138e-03,  5.2711e-03,  2.7841e-05, -3.1607e-04,\n",
      "          -1.1611e-05,  2.8154e-04],\n",
      "         [ 3.5571e-05, -8.3885e-05, -5.5350e-06,  2.8693e-05,  5.7946e-04,\n",
      "           7.6660e-04, -6.7503e-03,  5.4042e-03,  6.4715e-05,  8.7584e-04,\n",
      "           1.6595e-04, -8.5759e-04]]], device='cuda:0'),)\n",
      "Backward result of enc.layers.0.mhsa:\n",
      "(tensor([[[-0.0020,  0.0006, -0.0116,  0.0130],\n",
      "         [ 0.0014,  0.0015, -0.0055,  0.0026],\n",
      "         [ 0.0010,  0.0010, -0.0052,  0.0033],\n",
      "         [ 0.0012,  0.0003, -0.0052,  0.0037],\n",
      "         [ 0.0011,  0.0001, -0.0038,  0.0025]]], device='cuda:0'),)\n",
      "Backward result of enc.layers.0:\n",
      "(tensor([[[ 0.0012, -0.0009, -0.0047,  0.0087],\n",
      "         [ 0.0004, -0.0014, -0.0030,  0.0059],\n",
      "         [ 0.0005, -0.0014, -0.0033,  0.0064],\n",
      "         [ 0.0006, -0.0014, -0.0036,  0.0070],\n",
      "         [ 0.0005, -0.0014, -0.0034,  0.0068]]], device='cuda:0'),)\n",
      "Backward result of enc:\n",
      "(tensor([[[ 0.0012, -0.0009, -0.0047,  0.0087],\n",
      "         [ 0.0004, -0.0014, -0.0030,  0.0059],\n",
      "         [ 0.0005, -0.0014, -0.0033,  0.0064],\n",
      "         [ 0.0006, -0.0014, -0.0036,  0.0070],\n",
      "         [ 0.0005, -0.0014, -0.0034,  0.0068]]], device='cuda:0'),)\n",
      "Backward result of pos:\n",
      "(tensor([[[ 0.0367, -0.0400, -0.0061,  0.0174],\n",
      "         [ 0.0092, -0.0161,  0.0177, -0.0059]]], device='cuda:0'),)\n",
      "Backward result of pos:\n",
      "(tensor([[[-0.0034,  0.0012, -0.0107,  0.0141],\n",
      "         [-0.0012,  0.0027, -0.0038,  0.0042],\n",
      "         [-0.0022,  0.0027, -0.0034,  0.0047],\n",
      "         [-0.0015,  0.0016, -0.0036,  0.0051],\n",
      "         [-0.0011,  0.0009, -0.0021,  0.0044]]], device='cuda:0'),)\n",
      "Backward result of y_pre:\n",
      "(tensor([[[ 0.0367, -0.0400, -0.0061,  0.0174],\n",
      "         [ 0.0092, -0.0161,  0.0177, -0.0059]]], device='cuda:0'),)\n",
      "Backward result of y_pre.linear:\n",
      "(tensor([[[ 0.0367, -0.0400, -0.0061,  0.0174],\n",
      "         [ 0.0092, -0.0161,  0.0177, -0.0059]]], device='cuda:0'),)\n",
      "Backward result of x_pre:\n",
      "(tensor([[[-0.0034,  0.0012, -0.0107,  0.0141],\n",
      "         [-0.0012,  0.0027, -0.0038,  0.0042],\n",
      "         [-0.0022,  0.0027, -0.0034,  0.0047],\n",
      "         [-0.0015,  0.0016, -0.0036,  0.0051],\n",
      "         [-0.0011,  0.0009, -0.0021,  0.0044]]], device='cuda:0'),)\n",
      "Backward result of x_pre.linear:\n",
      "(tensor([[[-0.0034,  0.0012, -0.0107,  0.0141],\n",
      "         [-0.0012,  0.0027, -0.0038,  0.0042],\n",
      "         [-0.0022,  0.0027, -0.0034,  0.0047],\n",
      "         [-0.0015,  0.0016, -0.0036,  0.0051],\n",
      "         [-0.0011,  0.0009, -0.0021,  0.0044]]], device='cuda:0'),)\n",
      "epoch : 0, itr :0, MSE loss : 0.09996557235717773, RMSE loss : 0.3161733150482178\n"
     ]
    }
   ],
   "source": [
    "train_loss_log = []\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(1):\n",
    "  tgt_log_train = np.zeros((1,1))\n",
    "  pred_log_train = np.zeros((1,1))\n",
    "\n",
    "  for iter, (x, y, tgt) in enumerate(dataloader):\n",
    "    x, y, tgt = x.to(device), y.to(device), tgt.to(device)\n",
    "    tgt = tgt[:,:,0]\n",
    "    net.train()\n",
    "            \n",
    "    out = net(x, y)\n",
    "    loss = criterion(out, tgt)\n",
    "            \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    " \n",
    "\n",
    "    train_loss_log.append(loss.item())\n",
    "            \n",
    "    print(\"epoch : {}, itr :{}, MSE loss : {}, RMSE loss : {}\".format(epoch, iter, loss.item(), torch.sqrt(loss).item()))\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('x_pre.linear.weight', tensor([[-0.2682],\n",
      "        [ 0.4051],\n",
      "        [-0.3793],\n",
      "        [-0.9806]], device='cuda:0')), ('x_pre.linear.bias', tensor([ 0.3156, -0.6108,  0.9014,  0.3772], device='cuda:0')), ('y_pre.linear.weight', tensor([[0.6352],\n",
      "        [0.5144],\n",
      "        [0.4985],\n",
      "        [0.3749]], device='cuda:0')), ('y_pre.linear.bias', tensor([-0.4876, -0.8650,  0.8131,  0.8349], device='cuda:0')), ('pos.pe', tensor([[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
      "        [ 0.8415,  0.5403,  0.0100,  0.9999],\n",
      "        [ 0.9093, -0.4161,  0.0200,  0.9998],\n",
      "        [ 0.1411, -0.9900,  0.0300,  0.9996],\n",
      "        [-0.7568, -0.6536,  0.0400,  0.9992],\n",
      "        [-0.9589,  0.2837,  0.0500,  0.9988],\n",
      "        [-0.2794,  0.9602,  0.0600,  0.9982],\n",
      "        [ 0.6570,  0.7539,  0.0699,  0.9976],\n",
      "        [ 0.9894, -0.1455,  0.0799,  0.9968],\n",
      "        [ 0.4121, -0.9111,  0.0899,  0.9960],\n",
      "        [-0.5440, -0.8391,  0.0998,  0.9950],\n",
      "        [-1.0000,  0.0044,  0.1098,  0.9940],\n",
      "        [-0.5366,  0.8439,  0.1197,  0.9928],\n",
      "        [ 0.4202,  0.9074,  0.1296,  0.9916],\n",
      "        [ 0.9906,  0.1367,  0.1395,  0.9902],\n",
      "        [ 0.6503, -0.7597,  0.1494,  0.9888],\n",
      "        [-0.2879, -0.9577,  0.1593,  0.9872],\n",
      "        [-0.9614, -0.2752,  0.1692,  0.9856],\n",
      "        [-0.7510,  0.6603,  0.1790,  0.9838],\n",
      "        [ 0.1499,  0.9887,  0.1889,  0.9820],\n",
      "        [ 0.9129,  0.4081,  0.1987,  0.9801],\n",
      "        [ 0.8367, -0.5477,  0.2085,  0.9780],\n",
      "        [-0.0089, -1.0000,  0.2182,  0.9759],\n",
      "        [-0.8462, -0.5328,  0.2280,  0.9737],\n",
      "        [-0.9056,  0.4242,  0.2377,  0.9713],\n",
      "        [-0.1324,  0.9912,  0.2474,  0.9689],\n",
      "        [ 0.7626,  0.6469,  0.2571,  0.9664],\n",
      "        [ 0.9564, -0.2921,  0.2667,  0.9638],\n",
      "        [ 0.2709, -0.9626,  0.2764,  0.9611],\n",
      "        [-0.6636, -0.7481,  0.2860,  0.9582],\n",
      "        [-0.9880,  0.1543,  0.2955,  0.9553],\n",
      "        [-0.4040,  0.9147,  0.3051,  0.9523],\n",
      "        [ 0.5514,  0.8342,  0.3146,  0.9492],\n",
      "        [ 0.9999, -0.0133,  0.3240,  0.9460],\n",
      "        [ 0.5291, -0.8486,  0.3335,  0.9428],\n",
      "        [-0.4282, -0.9037,  0.3429,  0.9394],\n",
      "        [-0.9918, -0.1280,  0.3523,  0.9359],\n",
      "        [-0.6435,  0.7654,  0.3616,  0.9323],\n",
      "        [ 0.2964,  0.9551,  0.3709,  0.9287],\n",
      "        [ 0.9638,  0.2666,  0.3802,  0.9249],\n",
      "        [ 0.7451, -0.6669,  0.3894,  0.9211],\n",
      "        [-0.1586, -0.9873,  0.3986,  0.9171],\n",
      "        [-0.9165, -0.4000,  0.4078,  0.9131],\n",
      "        [-0.8318,  0.5551,  0.4169,  0.9090],\n",
      "        [ 0.0177,  0.9998,  0.4259,  0.9048],\n",
      "        [ 0.8509,  0.5253,  0.4350,  0.9004],\n",
      "        [ 0.9018, -0.4322,  0.4439,  0.8961],\n",
      "        [ 0.1236, -0.9923,  0.4529,  0.8916],\n",
      "        [-0.7683, -0.6401,  0.4618,  0.8870],\n",
      "        [-0.9538,  0.3006,  0.4706,  0.8823],\n",
      "        [-0.2624,  0.9650,  0.4794,  0.8776],\n",
      "        [ 0.6702,  0.7422,  0.4882,  0.8727],\n",
      "        [ 0.9866, -0.1630,  0.4969,  0.8678],\n",
      "        [ 0.3959, -0.9183,  0.5055,  0.8628],\n",
      "        [-0.5588, -0.8293,  0.5141,  0.8577],\n",
      "        [-0.9998,  0.0221,  0.5227,  0.8525],\n",
      "        [-0.5216,  0.8532,  0.5312,  0.8473],\n",
      "        [ 0.4362,  0.8999,  0.5396,  0.8419],\n",
      "        [ 0.9929,  0.1192,  0.5480,  0.8365],\n",
      "        [ 0.6367, -0.7711,  0.5564,  0.8309],\n",
      "        [-0.3048, -0.9524,  0.5646,  0.8253],\n",
      "        [-0.9661, -0.2581,  0.5729,  0.8196],\n",
      "        [-0.7392,  0.6735,  0.5810,  0.8139],\n",
      "        [ 0.1674,  0.9859,  0.5891,  0.8080],\n",
      "        [ 0.9200,  0.3919,  0.5972,  0.8021],\n",
      "        [ 0.8268, -0.5625,  0.6052,  0.7961],\n",
      "        [-0.0266, -0.9996,  0.6131,  0.7900],\n",
      "        [-0.8555, -0.5178,  0.6210,  0.7838],\n",
      "        [-0.8979,  0.4401,  0.6288,  0.7776],\n",
      "        [-0.1148,  0.9934,  0.6365,  0.7712],\n",
      "        [ 0.7739,  0.6333,  0.6442,  0.7648],\n",
      "        [ 0.9511, -0.3090,  0.6518,  0.7584],\n",
      "        [ 0.2538, -0.9673,  0.6594,  0.7518],\n",
      "        [-0.6768, -0.7362,  0.6669,  0.7452],\n",
      "        [-0.9851,  0.1717,  0.6743,  0.7385],\n",
      "        [-0.3878,  0.9218,  0.6816,  0.7317],\n",
      "        [ 0.5661,  0.8243,  0.6889,  0.7248],\n",
      "        [ 0.9995, -0.0310,  0.6961,  0.7179],\n",
      "        [ 0.5140, -0.8578,  0.7033,  0.7109],\n",
      "        [-0.4441, -0.8960,  0.7104,  0.7038],\n",
      "        [-0.9939, -0.1104,  0.7174,  0.6967],\n",
      "        [-0.6299,  0.7767,  0.7243,  0.6895],\n",
      "        [ 0.3132,  0.9497,  0.7311,  0.6822],\n",
      "        [ 0.9684,  0.2495,  0.7379,  0.6749],\n",
      "        [ 0.7332, -0.6800,  0.7446,  0.6675],\n",
      "        [-0.1761, -0.9844,  0.7513,  0.6600],\n",
      "        [-0.9235, -0.3837,  0.7578,  0.6524],\n",
      "        [-0.8218,  0.5698,  0.7643,  0.6448],\n",
      "        [ 0.0354,  0.9994,  0.7707,  0.6372],\n",
      "        [ 0.8601,  0.5102,  0.7771,  0.6294],\n",
      "        [ 0.8940, -0.4481,  0.7833,  0.6216],\n",
      "        [ 0.1060, -0.9944,  0.7895,  0.6137],\n",
      "        [-0.7795, -0.6264,  0.7956,  0.6058],\n",
      "        [-0.9483,  0.3174,  0.8016,  0.5978],\n",
      "        [-0.2453,  0.9695,  0.8076,  0.5898],\n",
      "        [ 0.6833,  0.7302,  0.8134,  0.5817],\n",
      "        [ 0.9836, -0.1804,  0.8192,  0.5735],\n",
      "        [ 0.3796, -0.9251,  0.8249,  0.5653],\n",
      "        [-0.5734, -0.8193,  0.8305,  0.5570],\n",
      "        [-0.9992,  0.0398,  0.8360,  0.5487]], device='cuda:0')), ('enc.layers.0.mhsa.to_qvk.weight', tensor([[-0.0764,  0.2978,  0.3594, -0.1340],\n",
      "        [ 0.1949, -0.1657,  0.1682, -0.0049],\n",
      "        [-0.1861,  0.2763,  0.2457,  0.0009],\n",
      "        [-0.0875, -0.3292,  0.0721, -0.4952],\n",
      "        [ 0.1432, -0.0479, -0.2950, -0.3609],\n",
      "        [ 0.1961, -0.4319, -0.3200, -0.0140],\n",
      "        [ 0.1064, -0.3833, -0.3158, -0.1260],\n",
      "        [-0.3812, -0.1962, -0.0076,  0.1665],\n",
      "        [-0.0459,  0.4190,  0.0781, -0.4970],\n",
      "        [ 0.3747, -0.2191,  0.1452,  0.0655],\n",
      "        [-0.1429, -0.4419, -0.3451,  0.1499],\n",
      "        [-0.0829,  0.2136,  0.0622, -0.3256]], device='cuda:0')), ('enc.layers.0.mhsa.to_qvk.bias', tensor([-0.0565,  0.4533,  0.0063, -0.1738,  0.4815,  0.1465,  0.2029, -0.1838,\n",
      "        -0.4289,  0.3744,  0.3535,  0.0467], device='cuda:0')), ('enc.layers.0.ln1.weight', tensor([1.0000, 1.0001, 1.0001, 0.9998], device='cuda:0')), ('enc.layers.0.ln1.bias', tensor([-7.9409e-05, -6.1446e-05,  2.3467e-04, -2.3982e-04], device='cuda:0')), ('enc.layers.0.ff.linear1.weight', tensor([[ 0.4768,  0.4949, -0.4620,  0.1607],\n",
      "        [ 0.1838, -0.0345,  0.1893, -0.3263],\n",
      "        [-0.4538, -0.3456,  0.0465, -0.1982]], device='cuda:0')), ('enc.layers.0.ff.linear1.bias', tensor([ 0.0738,  0.2363, -0.1837], device='cuda:0')), ('enc.layers.0.ff.linear2.weight', tensor([[-0.2365, -0.1237,  0.4150],\n",
      "        [-0.5161, -0.2100, -0.5307],\n",
      "        [ 0.0281, -0.4580,  0.2138],\n",
      "        [ 0.0866, -0.2388, -0.4672]], device='cuda:0')), ('enc.layers.0.ff.linear2.bias', tensor([-0.3809, -0.0024,  0.1792,  0.0414], device='cuda:0')), ('enc.layers.0.ln2.weight', tensor([1.0000, 0.9999, 1.0001, 0.9997], device='cuda:0')), ('enc.layers.0.ln2.bias', tensor([-3.1706e-05,  6.5240e-05,  1.7928e-04, -3.4869e-04], device='cuda:0')), ('dec.layers.0.mhca.to_qvk.weight', tensor([[ 0.0379, -0.1005,  0.0657, -0.0074],\n",
      "        [ 0.4559,  0.4950, -0.4763,  0.3770],\n",
      "        [ 0.3806,  0.4240, -0.0830, -0.2643],\n",
      "        [ 0.2191, -0.1465,  0.4139, -0.4185],\n",
      "        [ 0.1254, -0.0970,  0.2130, -0.4011],\n",
      "        [-0.1882,  0.0863,  0.2670, -0.3929],\n",
      "        [ 0.4080,  0.1087, -0.3415, -0.0510],\n",
      "        [-0.3877,  0.2640,  0.2076, -0.0888],\n",
      "        [ 0.2840,  0.0566, -0.0864, -0.4940],\n",
      "        [-0.1811, -0.4052,  0.3763,  0.4005],\n",
      "        [-0.2731, -0.2079, -0.1680, -0.4867],\n",
      "        [-0.0972, -0.0581, -0.4126,  0.4835]], device='cuda:0')), ('dec.layers.0.mhca.to_qvk.bias', tensor([-0.4087, -0.0889,  0.1709,  0.4121, -0.3657, -0.1199, -0.1339, -0.1922,\n",
      "         0.0753, -0.4092, -0.1540,  0.3804], device='cuda:0')), ('dec.layers.0.ln1.weight', tensor([1.0003, 0.9997, 1.0000, 1.0000], device='cuda:0')), ('dec.layers.0.ln1.bias', tensor([-2.7769e-04,  6.5207e-04, -3.7122e-04, -3.3706e-05], device='cuda:0')), ('dec.layers.0.mhsa.to_kv.weight', tensor([[ 0.2841,  0.4436, -0.1006, -0.0301],\n",
      "        [ 0.1843,  0.1944, -0.0551,  0.1103],\n",
      "        [ 0.0151, -0.0109, -0.2432,  0.2961],\n",
      "        [-0.4394, -0.2283,  0.0326,  0.3265],\n",
      "        [ 0.2907,  0.1759, -0.3367,  0.2571],\n",
      "        [ 0.2996,  0.3450,  0.3922, -0.2921],\n",
      "        [ 0.3865,  0.2868,  0.4062,  0.2637],\n",
      "        [ 0.3821,  0.3360,  0.3989, -0.1817]], device='cuda:0')), ('dec.layers.0.mhsa.to_kv.bias', tensor([ 0.3991, -0.2893, -0.4332,  0.4100,  0.2213,  0.2794, -0.3244,  0.3214],\n",
      "       device='cuda:0')), ('dec.layers.0.mhsa.to_q.weight', tensor([[-0.4688, -0.2195, -0.0746, -0.2067],\n",
      "        [ 0.2856,  0.4543,  0.2394, -0.1444],\n",
      "        [-0.0257, -0.2001, -0.1581, -0.1731],\n",
      "        [ 0.4009, -0.1401,  0.1241,  0.0197]], device='cuda:0')), ('dec.layers.0.mhsa.to_q.bias', tensor([-0.1663, -0.2557, -0.3440,  0.2109], device='cuda:0')), ('dec.layers.0.ln2.weight', tensor([1.0003, 0.9997, 1.0000, 0.9996], device='cuda:0')), ('dec.layers.0.ln2.bias', tensor([-0.0002,  0.0007, -0.0004, -0.0002], device='cuda:0')), ('dec.layers.0.ff.linear1.weight', tensor([[ 0.2631, -0.3788,  0.1737, -0.3783],\n",
      "        [-0.2449, -0.1368,  0.1374,  0.4379],\n",
      "        [ 0.3585, -0.1431,  0.2044,  0.2547]], device='cuda:0')), ('dec.layers.0.ff.linear1.bias', tensor([ 0.4331,  0.2420, -0.3066], device='cuda:0')), ('dec.layers.0.ff.linear2.weight', tensor([[ 0.3878,  0.5670, -0.1952],\n",
      "        [ 0.2749,  0.1386, -0.4101],\n",
      "        [ 0.5297, -0.1220,  0.4580],\n",
      "        [ 0.3199,  0.0374,  0.4590]], device='cuda:0')), ('dec.layers.0.ff.linear2.bias', tensor([ 0.4909,  0.2975, -0.2332, -0.3477], device='cuda:0')), ('dec.layers.0.ln3.weight', tensor([1.0001, 0.9980, 0.9979, 0.9975], device='cuda:0')), ('dec.layers.0.ln3.bias', tensor([ 0.0003,  0.0026,  0.0022, -0.0017], device='cuda:0')), ('post.linear.weight', tensor([[ 0.0607,  0.4829,  0.3969, -0.3069]], device='cuda:0')), ('post.linear.bias', tensor([-0.0580], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "after_backprop = net.state_dict()\n",
    "print(after_backprop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient name: x_pre.linear.weight\n",
      "Gradient value:\n",
      "tensor([[ 0.0001],\n",
      "        [-0.0038],\n",
      "        [-0.0029],\n",
      "        [ 0.0027]], device='cuda:0')\n",
      "\n",
      "Gradient name: x_pre.linear.bias\n",
      "Gradient value:\n",
      "tensor([-0.0094,  0.0090, -0.0236,  0.0324], device='cuda:0')\n",
      "\n",
      "Gradient name: y_pre.linear.weight\n",
      "Gradient value:\n",
      "tensor([[-0.0458],\n",
      "        [ 0.0560],\n",
      "        [-0.0115],\n",
      "        [-0.0116]], device='cuda:0')\n",
      "\n",
      "Gradient name: y_pre.linear.bias\n",
      "Gradient value:\n",
      "tensor([ 0.0458, -0.0561,  0.0116,  0.0115], device='cuda:0')\n",
      "\n",
      "Gradient name: enc.layers.0.mhsa.to_qvk.weight\n",
      "Gradient value:\n",
      "tensor([[ 2.6829e-05, -4.7889e-04, -1.1183e-04,  2.5020e-05],\n",
      "        [-2.6730e-04,  3.2727e-04, -5.0506e-04, -7.6907e-04],\n",
      "        [ 2.9824e-05,  2.8572e-04,  1.9906e-04,  1.5080e-04],\n",
      "        [ 4.4530e-05, -2.8407e-04,  4.4842e-06,  1.0558e-04],\n",
      "        [ 2.1313e-03, -2.9665e-03,  3.0108e-03,  5.1463e-03],\n",
      "        [ 2.6034e-03, -3.6856e-03,  3.9202e-03,  6.5806e-03],\n",
      "        [-2.2524e-02,  3.1925e-02, -3.4498e-02, -5.7607e-02],\n",
      "        [ 1.7790e-02, -2.5273e-02,  2.7567e-02,  4.5880e-02],\n",
      "        [-6.8148e-06, -2.5035e-04,  5.7485e-05,  1.3891e-04],\n",
      "        [-1.2469e-03,  3.5192e-05,  1.4443e-04,  3.4320e-04],\n",
      "        [-1.5853e-04, -2.2882e-04,  7.0094e-05,  1.6851e-04],\n",
      "        [ 1.1580e-03,  1.2568e-04, -1.7479e-04, -4.1770e-04]], device='cuda:0')\n",
      "\n",
      "Gradient name: enc.layers.0.mhsa.to_qvk.bias\n",
      "Gradient value:\n",
      "tensor([-3.3752e-04, -5.3816e-04,  3.8168e-04, -1.0573e-04,  2.6053e-03,\n",
      "         3.5185e-03, -3.1284e-02,  2.5160e-02,  9.0949e-12, -1.1642e-10,\n",
      "         3.5470e-11,  2.9104e-11], device='cuda:0')\n",
      "\n",
      "Gradient name: enc.layers.0.ln1.weight\n",
      "Gradient value:\n",
      "tensor([-0.0025, -0.0062, -0.0103,  0.0190], device='cuda:0')\n",
      "\n",
      "Gradient name: enc.layers.0.ln1.bias\n",
      "Gradient value:\n",
      "tensor([ 0.0079,  0.0061, -0.0235,  0.0240], device='cuda:0')\n",
      "\n",
      "Gradient name: enc.layers.0.ff.linear1.weight\n",
      "Gradient value:\n",
      "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [-5.7261e-05, -3.4779e-03,  1.1550e-03,  2.3802e-03],\n",
      "        [ 8.9214e-03,  9.9640e-03, -6.3802e-03, -1.2505e-02]], device='cuda:0')\n",
      "\n",
      "Gradient name: enc.layers.0.ff.linear1.bias\n",
      "Gradient value:\n",
      "tensor([ 0.0000,  0.0022, -0.0153], device='cuda:0')\n",
      "\n",
      "Gradient name: enc.layers.0.ff.linear2.weight\n",
      "Gradient value:\n",
      "tensor([[ 0.0000e+00,  1.3515e-05,  4.0805e-04],\n",
      "        [ 0.0000e+00,  3.6754e-05,  7.8170e-05],\n",
      "        [ 0.0000e+00, -3.1682e-04, -4.1780e-03],\n",
      "        [ 0.0000e+00,  2.6655e-04,  3.6918e-03]], device='cuda:0')\n",
      "\n",
      "Gradient name: enc.layers.0.ff.linear2.bias\n",
      "Gradient value:\n",
      "tensor([ 0.0006,  0.0009, -0.0232,  0.0217], device='cuda:0')\n",
      "\n",
      "Gradient name: enc.layers.0.ln2.weight\n",
      "Gradient value:\n",
      "tensor([-0.0027,  0.0068, -0.0114,  0.0292], device='cuda:0')\n",
      "\n",
      "Gradient name: enc.layers.0.ln2.bias\n",
      "Gradient value:\n",
      "tensor([ 0.0032, -0.0065, -0.0179,  0.0349], device='cuda:0')\n",
      "\n",
      "Gradient name: dec.layers.0.mhca.to_qvk.weight\n",
      "Gradient value:\n",
      "tensor([[-1.0789e-04, -3.2619e-04,  1.2758e-04,  5.6956e-04],\n",
      "        [-1.9211e-05, -5.8080e-05,  2.2716e-05,  1.0141e-04],\n",
      "        [ 7.0671e-05,  2.1366e-04, -8.3564e-05, -3.7307e-04],\n",
      "        [ 3.0460e-05,  9.2089e-05, -3.6017e-05, -1.6079e-04],\n",
      "        [-2.2559e-02, -7.8260e-03,  6.3891e-03,  2.9621e-02],\n",
      "        [ 5.2784e-02,  2.3819e-02, -1.6800e-02, -7.7468e-02],\n",
      "        [-2.5806e-02, -1.8081e-02,  1.0376e-02,  4.7411e-02],\n",
      "        [-4.4185e-03,  2.0882e-03,  3.4802e-05,  4.3606e-04],\n",
      "        [-5.0338e-04,  2.7221e-04, -7.5668e-06, -1.1805e-06],\n",
      "        [-3.6390e-04,  1.9679e-04, -5.4701e-06, -8.5343e-07],\n",
      "        [-1.0916e-03,  5.9030e-04, -1.6409e-05, -2.5598e-06],\n",
      "        [-3.6312e-06,  1.9637e-06, -5.4585e-08, -8.5160e-09]], device='cuda:0')\n",
      "\n",
      "Gradient name: dec.layers.0.mhca.to_qvk.bias\n",
      "Gradient value:\n",
      "tensor([ 3.8952e-04,  6.9356e-05, -2.5514e-04, -1.0997e-04,  2.0285e-02,\n",
      "        -5.3041e-02,  3.2451e-02,  3.0517e-04, -5.8208e-11, -2.9104e-11,\n",
      "         0.0000e+00, -4.5475e-13], device='cuda:0')\n",
      "\n",
      "Gradient name: dec.layers.0.ln1.weight\n",
      "Gradient value:\n",
      "tensor([-0.0347,  0.0284,  0.0012,  0.0045], device='cuda:0')\n",
      "\n",
      "Gradient name: dec.layers.0.ln1.bias\n",
      "Gradient value:\n",
      "tensor([ 0.0278, -0.0652,  0.0371,  0.0034], device='cuda:0')\n",
      "\n",
      "Gradient name: dec.layers.0.mhsa.to_kv.weight\n",
      "Gradient value:\n",
      "tensor([[ 6.7796e-04, -1.4200e-03,  7.6968e-05,  6.6505e-04],\n",
      "        [-5.5850e-03,  1.1817e-02, -7.2262e-04, -5.5095e-03],\n",
      "        [-2.7040e-03,  5.7032e-03, -3.3653e-04, -2.6627e-03],\n",
      "        [-6.6794e-04,  1.3780e-03, -6.0217e-05, -6.4980e-04],\n",
      "        [-1.7207e-02, -2.1242e-02,  1.6318e-02,  2.2131e-02],\n",
      "        [ 4.3660e-02,  5.5606e-02, -4.1977e-02, -5.7288e-02],\n",
      "        [-2.4392e-02, -3.2601e-02,  2.3967e-02,  3.3025e-02],\n",
      "        [-2.0613e-03, -1.7627e-03,  1.6921e-03,  2.1319e-03]], device='cuda:0')\n",
      "\n",
      "Gradient name: dec.layers.0.mhsa.to_kv.bias\n",
      "Gradient value:\n",
      "tensor([ 5.8208e-11, -4.3656e-10, -4.3656e-11,  1.8190e-12,  2.5390e-02,\n",
      "        -6.5120e-02,  3.7009e-02,  2.7218e-03], device='cuda:0')\n",
      "\n",
      "Gradient name: dec.layers.0.mhsa.to_q.weight\n",
      "Gradient value:\n",
      "tensor([[ 4.1108e-03,  1.9216e-03, -2.0310e-05, -6.0121e-03],\n",
      "        [ 7.3269e-04,  3.5347e-04, -4.5510e-06, -1.0816e-03],\n",
      "        [-1.7846e-03, -8.0308e-04,  6.1716e-06,  2.5815e-03],\n",
      "        [-2.1772e-03, -1.0234e-03,  1.1234e-05,  3.1894e-03]], device='cuda:0')\n",
      "\n",
      "Gradient name: dec.layers.0.mhsa.to_q.bias\n",
      "Gradient value:\n",
      "tensor([-0.0038, -0.0007,  0.0017,  0.0020], device='cuda:0')\n",
      "\n",
      "Gradient name: dec.layers.0.ln2.weight\n",
      "Gradient value:\n",
      "tensor([-0.0281,  0.0339, -0.0039,  0.0373], device='cuda:0')\n",
      "\n",
      "Gradient name: dec.layers.0.ln2.bias\n",
      "Gradient value:\n",
      "tensor([ 0.0232, -0.0700,  0.0412,  0.0231], device='cuda:0')\n",
      "\n",
      "Gradient name: dec.layers.0.ff.linear1.weight\n",
      "Gradient value:\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0003,  0.0113, -0.0002, -0.0108],\n",
      "        [-0.0178, -0.0258, -0.0021,  0.0456]], device='cuda:0')\n",
      "\n",
      "Gradient name: dec.layers.0.ff.linear1.bias\n",
      "Gradient value:\n",
      "tensor([ 0.0000, -0.0063,  0.0277], device='cuda:0')\n",
      "\n",
      "Gradient name: dec.layers.0.ff.linear2.weight\n",
      "Gradient value:\n",
      "tensor([[ 0.0000e+00,  1.4640e-02, -3.3455e-05],\n",
      "        [ 0.0000e+00, -8.3006e-02, -4.1746e-05],\n",
      "        [ 0.0000e+00,  4.5106e-02,  5.6698e-05],\n",
      "        [ 0.0000e+00,  2.3260e-02,  1.8503e-05]], device='cuda:0')\n",
      "\n",
      "Gradient name: dec.layers.0.ff.linear2.bias\n",
      "Gradient value:\n",
      "tensor([ 0.0117, -0.0669,  0.0364,  0.0188], device='cuda:0')\n",
      "\n",
      "Gradient name: dec.layers.0.ln3.weight\n",
      "Gradient value:\n",
      "tensor([-0.0085,  0.2022,  0.2101,  0.2501], device='cuda:0')\n",
      "\n",
      "Gradient name: dec.layers.0.ln3.bias\n",
      "Gradient value:\n",
      "tensor([-0.0318, -0.2618, -0.2161,  0.1692], device='cuda:0')\n",
      "\n",
      "Gradient name: post.linear.weight\n",
      "Gradient value:\n",
      "tensor([[-0.1434,  0.4152,  0.5226, -0.7943]], device='cuda:0')\n",
      "\n",
      "Gradient name: post.linear.bias\n",
      "Gradient value:\n",
      "tensor([-0.5375], device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f'Gradient name: {name}')\n",
    "        print(f'Gradient value:\\n{param.grad}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
